{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31acb3c6",
   "metadata": {},
   "source": [
    "# Access to Healthy Assets and Hazards Input Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f8b7fe",
   "metadata": {},
   "source": [
    "\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea86817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (0.0.2)\n",
      "Requirement already satisfied: selenium in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (4.39.0)\n",
      "Requirement already satisfied: earthengine-api in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (1.7.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from bs4) (4.14.3)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.5.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Requirement already satisfied: outcome in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: google-cloud-storage in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (3.7.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.12.1 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (2.187.0)\n",
      "Requirement already satisfied: google-auth>=1.4.1 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (2.41.1)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (0.3.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (0.31.0)\n",
      "Requirement already satisfied: requests in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from earthengine-api) (2.32.5)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from httplib2<1dev,>=0.9.2->earthengine-api) (3.3.1)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (2.28.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-api-python-client>=1.12.1->earthengine-api) (4.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (6.33.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.12.1->earthengine-api) (1.27.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-auth>=1.4.1->earthengine-api) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-auth>=1.4.1->earthengine-api) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-auth>=1.4.1->earthengine-api) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from requests->earthengine-api) (3.4.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth>=1.4.1->earthengine-api) (0.6.1)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from beautifulsoup4->bs4) (2.8.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-cloud-storage->earthengine-api) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-cloud-storage->earthengine-api) (2.8.0)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in /Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages (from google-cloud-storage->earthengine-api) (1.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for this notebook:\n",
    "# - bs4: BeautifulSoup for HTML parsing\n",
    "# - selenium: browser automation (useful for JS-driven pages)\n",
    "# - earthengine-api: Google Earth Engine Python client (requires authentication)\n",
    "# Use the Jupyter %pip magic so packages are installed into the notebook environment.\n",
    "# After installing earthengine-api, run ee.Authenticate() and ee.Initialize() before using GEE.\n",
    "%pip install bs4 selenium earthengine-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1673b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path  # For handling file paths in a cross-platform way\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import geopandas as gpd  # For geospatial data handling\n",
    "import requests  # For making HTTP requests\n",
    "from bs4 import BeautifulSoup  # For parsing HTML content\n",
    "import re  # For regular expressions\n",
    "import requests  # Duplicate import (consider removing)\n",
    "import time  # For time-related functions\n",
    "from typing import Optional  # For type hints\n",
    "import io  # For in-memory I/O operations\n",
    "import os  # For operating system interfaces\n",
    "import tempfile  # For temporary file creation\n",
    "import zipfile  # For ZIP file handling\n",
    "import urllib.request  # For URL retrieval\n",
    "import ee  # For Google Earth Engine API\n",
    "import subprocess  # For running subprocesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd094f33",
   "metadata": {},
   "source": [
    "# Boundaries\n",
    "\n",
    "This creates a consolidated GB Boundary using 2021/22 Census definitions: Lower Layer Super Output Areas (LSOAs) for England and Wales and Data Zones (DZs) for Scotland.\n",
    "\n",
    "## England and Wales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9b490356",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = (\n",
    "    \"https://services1.arcgis.com/ESMARspQHYMw9BZ9/arcgis/rest/services/\"\n",
    "    \"Lower_layer_Super_Output_Areas_December_2021_Boundaries_EW_BGC_V5/FeatureServer/0/query\"\n",
    ")\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def fetch_chunk(offset: int, limit: int = 2000) -> gpd.GeoDataFrame:\n",
    "    params = {\n",
    "        \"where\": \"1=1\",\n",
    "        \"outFields\": \"LSOA21CD\",\n",
    "        \"outSR\": \"4326\",\n",
    "        \"f\": \"geojson\",\n",
    "        \"resultOffset\": offset,\n",
    "        \"resultRecordCount\": limit,\n",
    "    }\n",
    "    r = session.get(BASE_URL, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return gpd.read_file(io.BytesIO(r.content))\n",
    "\n",
    "chunks = []\n",
    "offset = 0\n",
    "limit = 2000  # typical polygon maxRecordCount\n",
    "\n",
    "while True:\n",
    "    g = fetch_chunk(offset, limit)\n",
    "    if g.empty:\n",
    "        break\n",
    "    chunks.append(g)\n",
    "    # if we got fewer than limit, we've hit the end\n",
    "    if len(g) < limit:\n",
    "        break\n",
    "    offset += limit\n",
    "\n",
    "gdf = gpd.GeoDataFrame(pd.concat(chunks, ignore_index=True))\n",
    "gdf = gdf.set_crs(epsg=4326, allow_override=True)\n",
    "gdf = gdf.rename(columns={\"LSOA21CD\": \"LSOA_DZ_SDZ_21_22\"})\n",
    "gdf = gdf.to_crs(epsg=27700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e90336",
   "metadata": {},
   "source": [
    "# Scotland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LSOA_DZ_SDZ_21_22</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S01013482</td>\n",
       "      <td>POLYGON ((383385 800703, 383385.411 800699.959...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S01013483</td>\n",
       "      <td>POLYGON ((383304.495 801654.205, 383391.947 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S01013484</td>\n",
       "      <td>POLYGON ((383473 801227, 383597 801087, 383598...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S01013485</td>\n",
       "      <td>POLYGON ((383976.659 801182.579, 383984.102 80...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S01013486</td>\n",
       "      <td>POLYGON ((384339 801211, 384316.51 801182.159,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>S01020870</td>\n",
       "      <td>POLYGON ((307423.889 672579.756, 307424.594 67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>S01020871</td>\n",
       "      <td>POLYGON ((308463.398 672524.398, 308451.095 67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>S01020872</td>\n",
       "      <td>POLYGON ((308734 672598, 308743 672543, 308905...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>S01020873</td>\n",
       "      <td>POLYGON ((309949.893 672738.667, 309970.518 67...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>S01019676</td>\n",
       "      <td>POLYGON ((357261.433 632996.42, 357253.1 63291...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7392 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LSOA_DZ_SDZ_21_22                                           geometry\n",
       "0            S01013482  POLYGON ((383385 800703, 383385.411 800699.959...\n",
       "1            S01013483  POLYGON ((383304.495 801654.205, 383391.947 80...\n",
       "2            S01013484  POLYGON ((383473 801227, 383597 801087, 383598...\n",
       "3            S01013485  POLYGON ((383976.659 801182.579, 383984.102 80...\n",
       "4            S01013486  POLYGON ((384339 801211, 384316.51 801182.159,...\n",
       "...                ...                                                ...\n",
       "7387         S01020870  POLYGON ((307423.889 672579.756, 307424.594 67...\n",
       "7388         S01020871  POLYGON ((308463.398 672524.398, 308451.095 67...\n",
       "7389         S01020872  POLYGON ((308734 672598, 308743 672543, 308905...\n",
       "7390         S01020873  POLYGON ((309949.893 672738.667, 309970.518 67...\n",
       "7391         S01019676  POLYGON ((357261.433 632996.42, 357253.1 63291...\n",
       "\n",
       "[7392 rows x 2 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a temporary directory and download the zip there\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# download and import Scottish Data Zone 2022 shapefile into sg_dz_gdf\n",
    "sg_zip_url = \"https://maps.gov.scot/ATOM/shapefiles/SG_DataZoneBdry_2022.zip\"\n",
    "sg_zip_path = os.path.join(tmp_dir, \"SG_DataZoneBdry_2022.zip\")\n",
    "urllib.request.urlretrieve(sg_zip_url, sg_zip_path)\n",
    "\n",
    "# extract\n",
    "with zipfile.ZipFile(sg_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(tmp_dir)\n",
    "\n",
    "# read\n",
    "sg_dz_gdf = gpd.read_file(os.path.join(tmp_dir, \"SG_DataZone_Bdry_2022.shp\"))\n",
    "\n",
    "sg_dz_gdf = sg_dz_gdf.rename(columns={\"dzcode\": \"LSOA_DZ_SDZ_21_22\"})\n",
    "sg_dz_gdf = sg_dz_gdf[[\"LSOA_DZ_SDZ_21_22\", \"geometry\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "709bfef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved boundaries geoparquet to data/boundary/LSOA_DZ_SDZ_21_22.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine gdf (England and Wales) and sg_dz_gdf (Scotland) into a single GeoDataFrame\n",
    "boundaries_all = gpd.GeoDataFrame(\n",
    "    pd.concat([gdf, sg_dz_gdf], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=gdf.crs\n",
    ")\n",
    "\n",
    "# Export to GeoParquet\n",
    "out_dir = Path(\"data\") / \"boundary\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"LSOA_DZ_SDZ_21_22.parquet\"\n",
    "boundaries_all.to_parquet(out_path, index=False)\n",
    "print(\"Saved boundaries geoparquet to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a8d436",
   "metadata": {},
   "source": [
    "# Import Postcodes\n",
    "\n",
    "Imports the ONSPD (Office for National Statistics Postcode Directory) data, and limits these to those within GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79e07fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,794,259 postcodes\n",
      "Converted 1,745,013 rows to GeoDataFrame (dropped 0 without coords)\n"
     ]
    }
   ],
   "source": [
    "# Load ONS Postcode Directory\n",
    "onspd_path = Path(\"data\") / \"raw_data\"/ \"ONSPD_NOV_2025_UK.csv\"\n",
    "\n",
    "# The ONSPD file is large, so we only load the columns we need\n",
    "# Key columns: pcds (postcode), oseast1m (easting), osnrth1m (northing)\n",
    "postcodes_raw = pd.read_csv(\n",
    "    onspd_path,\n",
    "    usecols=[\"pcd7\", \"lat\", \"long\",\"lsoa21cd\",\"doterm\"],\n",
    "    dtype={\"pcd7\": str, \"east1m\": float, \"north1m\": float}\n",
    ")\n",
    "\n",
    "# remove spaces from pcd7 and keep only rows where doterm is empty\n",
    "postcodes_raw[\"pcd7\"] = postcodes_raw[\"pcd7\"].str.replace(\" \", \"\", regex=False)\n",
    "mask = (\n",
    "    (postcodes_raw[\"doterm\"].isna() | (postcodes_raw[\"doterm\"].astype(str).str.strip() == \"\"))\n",
    "    & postcodes_raw[\"lsoa21cd\"].notna()\n",
    ")\n",
    "postcodes_raw = postcodes_raw.loc[mask].copy()\n",
    "\n",
    "# remove rows with invalid LSOA codes\n",
    "invalid_lsoa = {\"L99999999\", \"M99999999\"}\n",
    "postcodes_raw = postcodes_raw[~postcodes_raw[\"lsoa21cd\"].isin(invalid_lsoa)].copy()\n",
    "\n",
    "postcodes_raw = postcodes_raw.drop(columns=[\"doterm\"], errors=\"ignore\")\n",
    "print(f\"Loaded {len(postcodes_raw):,} postcodes\")\n",
    "postcodes_raw\n",
    "\n",
    "# remove rows where lsoa21cd starts with 'N'\n",
    "postcodes_raw = postcodes_raw[~postcodes_raw[\"lsoa21cd\"].astype(str).str.startswith(\"N\", na=False)].copy()\n",
    "\n",
    "# convert postcodes_raw to a GeoDataFrame\n",
    "postcodes_geo = postcodes_raw.dropna(subset=[\"lat\", \"long\"]).copy()\n",
    "postcodes_gdf = gpd.GeoDataFrame(\n",
    "    postcodes_geo,\n",
    "    geometry=gpd.points_from_xy(postcodes_geo[\"long\"], postcodes_geo[\"lat\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "# Report how many rows were converted\n",
    "print(f\"Converted {len(postcodes_gdf):,} rows to GeoDataFrame (dropped {len(postcodes_raw) - len(postcodes_gdf):,} without coords)\")\n",
    "\n",
    "# Save postcodes_gdf as GeoParquet\n",
    "postcode_out_dir = Path(\"data\") / \"postcodes\"\n",
    "postcode_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = postcode_out_dir / \"postcodes.parquet\"\n",
    "postcodes_gdf.to_parquet(out_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f59a6",
   "metadata": {},
   "source": [
    "# Import Retail Data\n",
    "\n",
    "Creates POI data for retail / leisure locations from the Local Data Company (LDC) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b472a5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 874,810 stores\n",
      "Converted 874,314 rows to GeoDataFrame (dropped 496 without coords)\n"
     ]
    }
   ],
   "source": [
    "# Load LDC Data\n",
    "LDC_path = Path(\"data\") / \"raw_data\" / \"LDC_Historical_snapshot_2015-01-01-2025-09-30.csv\"\n",
    "\n",
    "LDC_raw = pd.read_csv(\n",
    "    LDC_path,\n",
    "    usecols=[\"Name\", \"Latitude\", \"Longitude\",\"Status\",\"Category\",\"Subcategory\",\"Postcode\"],\n",
    "    dtype={\"Name\": str, \"Latitude\": float, \"Longitude\": float,\"Status\": str,\"Category\": str,\"Subcategory\": str},\n",
    ")\n",
    "\n",
    "LDC_raw = LDC_raw[LDC_raw[\"Status\"].str.strip().str.lower() == \"live\"].copy()\n",
    "\n",
    "# Load category to subcategory mapping\n",
    "category_subcategory = pd.read_csv(Path(\"data\") / \"raw_data\" /\"category_subcategory.csv\")\n",
    "\n",
    "# Append category_subcategory onto LDC_raw using Subcategory\n",
    "LDC_raw = LDC_raw.merge(\n",
    "    category_subcategory[[\"Subcategory\",\"Fast_Food\",\"Pub_Bar\",\"Gambling\",\"Tobacco\",\"Leisure\"]],\n",
    "    on=\"Subcategory\",\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(LDC_raw):,} stores\")\n",
    "\n",
    "# convert LDC_raw to a GeoDataFrame\n",
    "\n",
    "LDC_geo = LDC_raw.dropna(subset=[\"Latitude\", \"Longitude\"]).copy()\n",
    "LDC_gdf = gpd.GeoDataFrame(\n",
    "    LDC_geo,\n",
    "    geometry=gpd.points_from_xy(LDC_geo[\"Longitude\"], LDC_geo[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "print(f\"Converted {len(LDC_gdf):,} rows to GeoDataFrame (dropped {len(LDC_raw) - len(LDC_gdf):,} without coords)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc2088",
   "metadata": {},
   "source": [
    "### Create Retail / Leisure POI Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e2ff6382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GeoParquet files to data/retail\n"
     ]
    }
   ],
   "source": [
    "# Keep geometry so outputs are GeoParquet\n",
    "cols_to_keep = [\"Name\", \"Latitude\", \"Longitude\", \"Postcode\",\"geometry\"]\n",
    "\n",
    "def _subset_geo(flag_col: str) -> gpd.GeoDataFrame:\n",
    "    m = LDC_gdf[flag_col].fillna(0).astype(int).eq(1)\n",
    "    out = LDC_gdf.loc[m, cols_to_keep].copy()\n",
    "    return gpd.GeoDataFrame(out, geometry=\"geometry\", crs=LDC_gdf.crs)\n",
    "\n",
    "df_fast_food = _subset_geo(\"Fast_Food\")\n",
    "df_pub_bar = _subset_geo(\"Pub_Bar\")\n",
    "df_gambling = _subset_geo(\"Gambling\")\n",
    "df_tobacco = _subset_geo(\"Tobacco\")\n",
    "df_leisure = _subset_geo(\"Leisure\")\n",
    "\n",
    "# write outputs to data/retail (GeoParquet)\n",
    "out_dir = Path(\"data\") / \"retail\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_fast_food.to_parquet(out_dir / \"fast_food.parquet\", index=False)\n",
    "df_pub_bar.to_parquet(out_dir / \"pub_bar.parquet\", index=False)\n",
    "df_gambling.to_parquet(out_dir / \"gambling.parquet\", index=False)\n",
    "df_tobacco.to_parquet(out_dir / \"tobacco.parquet\", index=False)\n",
    "df_leisure.to_parquet(out_dir / \"leisure.parquet\", index=False)\n",
    "\n",
    "print(\"Saved GeoParquet files to\", out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944dfd2",
   "metadata": {},
   "source": [
    "# Health Services"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84246c54",
   "metadata": {},
   "source": [
    "## Hospitals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7295e0b1",
   "metadata": {},
   "source": [
    "## Scotland\n",
    "\n",
    "The hopital data for Scotland is not available from a direct download link (https://data.spatialhub.scot/dataset/nhs_hospitals-is/) and requires a log on to be created. The data are OGL, and have been placed in ./data/raw_data/NHS_Hospitals_-_Scotland.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "d18502e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "scot_path = Path(\"data\") / \"raw_data\" / \"NHS_Hospitals_-_Scotland.csv\"\n",
    "\n",
    "# Load only the required columns\n",
    "hospitals_scotland = pd.read_csv(\n",
    "    scot_path,\n",
    "    usecols=[\"sitename\", \"postcode\", \"x\", \"y\"],\n",
    "    low_memory=False,\n",
    "    dtype={\"sitename\": str, \"postcode\": str, \"x\": float, \"y\": float},\n",
    ")\n",
    "# Clean postcodes by removing spaces\n",
    "hospitals_scotland['postcode'] = hospitals_scotland['postcode'].str.replace(' ', '', regex=False)\n",
    "\n",
    "\n",
    "hospitals_scotland = hospitals_scotland.groupby([\"postcode\", \"x\", \"y\"], as_index=False).agg({\n",
    "    \"sitename\": lambda sitename: \" | \".join(sitename.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Create GeoDataFrame (assuming x,y are OSGB36 eastings/northings)\n",
    "hospitals_scotland = gpd.GeoDataFrame(\n",
    "    hospitals_scotland,\n",
    "    geometry=gpd.points_from_xy(hospitals_scotland[\"x\"], hospitals_scotland[\"y\"]),\n",
    "    crs=\"EPSG:27700\",\n",
    ")\n",
    "\n",
    "# Convert to WGS84 and add lat/lon, rename columns to match others\n",
    "hospitals_scotland = hospitals_scotland.to_crs(\"EPSG:4326\") # WGS84\n",
    "hospitals_scotland[\"Latitude\"] = hospitals_scotland.geometry.y # lat\n",
    "hospitals_scotland[\"Longitude\"] = hospitals_scotland.geometry.x # lon\n",
    "hospitals_scotland = hospitals_scotland.rename(columns={\"sitename\": \"Name\", \"postcode\": \"Postcode\"}) # rename columns\n",
    "hospitals_scotland = hospitals_scotland[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\", \"geometry\"]].copy() # keep only relevant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e62fa46",
   "metadata": {},
   "source": [
    "### England and Wales\n",
    "The hospital data for England and Wales is available as a direct download link from: https://digital.nhs.uk/services/organisation-data-service/data-search-and-export/csv-downloads/other-nhs-organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c14de8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for postcode BT126BA: 'Latitude'\n",
      "Error fetching data for postcode BT126BE: 'Latitude'\n",
      "Error fetching data for postcode BT97AB: 'Latitude'\n"
     ]
    }
   ],
   "source": [
    "# Import GP practice data for England and Wales\n",
    "hospitals_england_wales_url = \"https://www.odsdatasearchandexport.nhs.uk/api/getReport?report=ets\"\n",
    "hospitals_england_wales = pd.read_csv(hospitals_england_wales_url, header=None, low_memory=False, usecols=[1,9,12])\n",
    "\n",
    "hospitals_england_wales.to_csv(\"./data/raw_data/hospitals_england_wales.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "# Keep only rows where column 12 (Status) is empty (indicating active hospitals)\n",
    "hospitals_england_wales = hospitals_england_wales[hospitals_england_wales[12].isna()].copy()\n",
    "\n",
    "hospitals_england_wales.columns = ['Name', 'Postcode', 'Status']\n",
    "\n",
    "# Filter out rows where Postcode starts with 'JE' or 'IM' (Jersey / Isle of Man / Guernsey / British Forces)\n",
    "hospitals_england_wales = hospitals_england_wales[~hospitals_england_wales['Postcode'].str.startswith(('JE', 'IM','GY','BF'))]\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "hospitals_england_wales[\"Postcode\"] = hospitals_england_wales[\"Postcode\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "hospitals_england_wales = hospitals_england_wales.drop(columns=[\"Status\"], errors=\"ignore\")\n",
    "\n",
    "hospitals_england_wales = hospitals_england_wales.groupby(\"Postcode\", as_index=False).agg({\n",
    "    \"Name\": lambda names: \" | \".join(names.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Merge with postcodes on Postcode and pcd7\n",
    "hospitals_england_wales = hospitals_england_wales.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\"]],\n",
    "    left_on=\"Postcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename columns to match the standard format\n",
    "hospitals_england_wales = hospitals_england_wales.rename(columns={\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# Filter out rows where Postcode starts with 'JE' or 'IM' or 'GY' (Jersey / Isle of Man / Guernsey)\n",
    "hospitals_england_wales = hospitals_england_wales[~hospitals_england_wales['Postcode'].str.startswith(('JE', 'IM','GY'))]\n",
    "\n",
    "# Select final columns\n",
    "hospitals_england_wales = hospitals_england_wales[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\"]].copy()\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = hospitals_england_wales[hospitals_england_wales['Latitude'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = hospitals_england_wales.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            Latitude, Longitude = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                Latitude = data['result']['Latitude']\n",
    "                Longitude = data['result']['Longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                Latitude = data['terminated']['latitude']\n",
    "                Longitude = data['terminated']['longitude']\n",
    "            \n",
    "            if Latitude is not None and Longitude is not None:\n",
    "                hospitals_england_wales.at[idx, 'Latitude'] = Latitude\n",
    "                hospitals_england_wales.at[idx, 'Longitude'] = Longitude\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "# Remove rows where Latitude or Longitude are NaN\n",
    "hospitals_england_wales = hospitals_england_wales.dropna(subset=['Latitude', 'Longitude'])\n",
    "\n",
    "# Create GeoDataFrame \n",
    "hospitals_england_wales = gpd.GeoDataFrame(\n",
    "    hospitals_england_wales,\n",
    "    geometry=gpd.points_from_xy(hospitals_england_wales['Longitude'], hospitals_england_wales['Latitude']),\n",
    "    crs='EPSG:4326')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64208b9d",
   "metadata": {},
   "source": [
    "The folloiwing code creates a consolidated hospital POI dataset for GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0b322e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland hospitals: 295\n",
      "England and Wales hospitals: 17810\n",
      "Saved hospitals geoparquet to data/health/hospital.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine Scotland and England/Wales hospitals and export to GeoParquet\n",
    "\n",
    "print(f\"Scotland hospitals: {len(hospitals_scotland)}\")\n",
    "print(f\"England and Wales hospitals: {len(hospitals_england_wales)}\")\n",
    "\n",
    "hospitals_all = gpd.GeoDataFrame(\n",
    "    pd.concat([hospitals_england_wales, hospitals_scotland], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=hospitals_england_wales.crs\n",
    ")\n",
    "\n",
    "health_dir = Path(\"data\") / \"health\"\n",
    "health_dir.mkdir(parents=True, exist_ok=True)\n",
    "hospitals_out_path = health_dir / \"hospital.parquet\"\n",
    "hospitals_all.to_parquet(hospitals_out_path, index=False)\n",
    "print(\"Saved hospitals geoparquet to\", hospitals_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a433774",
   "metadata": {},
   "source": [
    "## GP Practices\n",
    "The following code creates a consolidated GP Practice POI dataset for GB. There is some noise in the data, with some practices having repeated entries. These were all treated as single locations. Furthermore, some practices have postcodes which are no longer current. These records were all retained but geocoded using postcodes.io, as the main postcode file imported earlier would not have these postcodes.\n",
    "### Scotland\n",
    "The GP practice data for Scotland is available from a direct download link (https://www.opendata.nhs.scot/dataset/gp-practice-contact-details-and-list-sizes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6a6f52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GP practice data for Scotland (Oct 2025)\n",
    "gp_scotland_url = \"https://www.opendata.nhs.scot/dataset/f23655c3-6e23-4103-a511-a80d998adb90/resource/47557411-7eda-4278-9d6d-d26ed2ceab5a/download/practice_contact_details_20251001_opendata.csv\"\n",
    "gp_scotland = pd.read_csv(gp_scotland_url, low_memory=False)\n",
    "\n",
    "gp_scotland.to_csv(\"./data/raw_data/gp_scotland_20251001_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "# Keep only relevant columns\n",
    "gp_scotland = gp_scotland[[\"GPPracticeName\", \"Postcode\"]]\n",
    "gp_scotland[\"Postcode\"] = gp_scotland[\"Postcode\"].str.replace(\" \", \"\", regex=False)\n",
    "\n",
    "# Aggregate by Postcode to combine multiple practices in the same postcode\n",
    "gp_scotland = gp_scotland.groupby(\"Postcode\", as_index=False).agg({\n",
    "    \"GPPracticeName\": lambda GPPracticeName: \" | \".join(GPPracticeName.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Join postcodes using a match between Postcode and pcd7\n",
    "gp_scotland = gp_scotland.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\"]],\n",
    "    left_on=\"Postcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename and select final columns\n",
    "gp_scotland = gp_scotland.rename(columns={\n",
    "    \"GPPracticeName\": \"Name\",\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = gp_scotland[gp_scotland['Latitude'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = gp_scotland.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            Latitude, Longitude = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                Latitude = data['result']['Latitude']\n",
    "                Longitude = data['result']['Longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                Latitude = data['terminated']['latitude']\n",
    "                Longitude = data['terminated']['longitude']\n",
    "            \n",
    "            if Latitude is not None and Longitude is not None:\n",
    "                gp_scotland.at[idx, 'Latitude'] = Latitude\n",
    "                gp_scotland.at[idx, 'Longitude'] = Longitude\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "# Select final columns\n",
    "gp_scotland = gp_scotland[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\"]]\n",
    "\n",
    "gp_scotland = gpd.GeoDataFrame(\n",
    "    gp_scotland,\n",
    "    geometry=gpd.points_from_xy(gp_scotland[\"Longitude\"], gp_scotland[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db4114",
   "metadata": {},
   "source": [
    "### England and Wales\n",
    "The GP practice data for England and Wales are available from a direct download link [(https://digital.nhs.uk/data-and-information/publications/statistical/general-and-personal-medical-services/2023-24-01-august-2023).](https://digital.nhs.uk/services/organisation-data-service/data-search-and-export/csv-downloads/gp-and-gp-practice-related-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b182de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GP practice data for England and Wales\n",
    "gp_england_wales_url = \"https://www.odsdatasearchandexport.nhs.uk/api/getReport?report=epraccur\"\n",
    "gp_england_wales = pd.read_csv(gp_england_wales_url, header=None, low_memory=False, usecols=[1,9,12])\n",
    "\n",
    "gp_england_wales.to_csv(\"./data/raw_data/gp_england_wales_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "gp_england_wales = gp_england_wales[gp_england_wales[12] == 'ACTIVE']\n",
    "\n",
    "gp_england_wales.columns = ['Name', 'Postcode', 'Status']\n",
    "\n",
    "\n",
    "# Filter out rows where Postcode starts with 'JE' or 'IM' or 'GY' (Jersey / Isle of Man / Guernsey) and British Forces\n",
    "gp_england_wales = gp_england_wales[~gp_england_wales['Postcode'].str.startswith(('JE', 'IM','GY','BF'))]\n",
    "\n",
    "gp_england_wales = gp_england_wales.drop(columns=[\"Status\"], errors=\"ignore\")\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "gp_england_wales[\"Postcode\"] = gp_england_wales[\"Postcode\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "\n",
    "gp_england_wales = gp_england_wales.groupby(\"Postcode\", as_index=False).agg({\n",
    "    \"Name\": lambda names: \" | \".join(names.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "\n",
    "# Merge with postcodes on Postcode and pcd7\n",
    "gp_england_wales = gp_england_wales.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\", \"geometry\"]],\n",
    "    left_on=\"Postcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename columns to match the standard format\n",
    "gp_england_wales = gp_england_wales.rename(columns={\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = gp_england_wales[gp_england_wales['Latitude'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = gp_england_wales.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            Latitude, Longitude = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                Latitude = data['result']['Latitude']\n",
    "                Longitude = data['result']['Longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                Latitude = data['terminated']['latitude']\n",
    "                Longitude = data['terminated']['longitude']\n",
    "            \n",
    "            if Latitude is not None and Longitude is not None:\n",
    "                gp_england_wales.at[idx, 'Latitude'] = Latitude\n",
    "                gp_england_wales.at[idx, 'Longitude'] = Longitude\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "\n",
    "# Select final columns\n",
    "gp_england_wales = gp_england_wales[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\"]].copy()\n",
    "\n",
    "gp_england_wales = gpd.GeoDataFrame(\n",
    "    gp_england_wales,\n",
    "    geometry=gpd.points_from_xy(gp_england_wales[\"Longitude\"], gp_england_wales[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1bc42a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland GPs: 706\n",
      "England and Wales GPs: 8423\n",
      "Saved GP practices geoparquet to data/health/GP.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine GP practices from England, Wales, and Scotland\n",
    "print(f\"Scotland GPs: {len(gp_scotland)}\")\n",
    "print(f\"England and Wales GPs: {len(gp_england_wales)}\")\n",
    "\n",
    "gp_practices = gpd.GeoDataFrame(\n",
    "    pd.concat([gp_england_wales, gp_scotland], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=postcodes.crs\n",
    ")   \n",
    "\n",
    "health_dir = Path(\"data\") / \"health\"\n",
    "health_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export combined GP practices as GeoParquet\n",
    "gp_out_path = health_dir / \"GP.parquet\"\n",
    "gp_practices.to_parquet(gp_out_path, index=False)\n",
    "print(\"Saved GP practices geoparquet to\", gp_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff9705",
   "metadata": {},
   "source": [
    "## Pharmacies\n",
    "### Scotland\n",
    "The pharmacy data for Scotland is available from a direct download - https://www.opendata.nhs.scot/dataset/dispenser-location-contact-details/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e3d8d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pharmacy data for Scotland\n",
    "pharmacy_scotland_url = \"https://www.opendata.nhs.scot/dataset/a30fde16-1226-49b3-b13d-eb90e39c2058/resource/fba47ad2-9082-4e4a-a70d-16215922c1f7/download/dispenser_contactdetails_jul-sep_2025-26.csv\"\n",
    "pharmacy_scotland = pd.read_csv(pharmacy_scotland_url, low_memory=False)\n",
    "\n",
    "pharmacy_scotland.to_csv(\"./data/raw_data/pharmacy_scotland_20251001_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "# Select relevant columns (assuming 'DispenserName' and 'Postcode' based on typical structure)\n",
    "pharmacy_scotland = pharmacy_scotland[[\"DispLocationName\", \"DispLocationPostcode\"]]\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "pharmacy_scotland[\"DispLocationPostcode\"] = pharmacy_scotland[\"DispLocationPostcode\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "\n",
    "# Group by postcode to aggregate names\n",
    "pharmacy_scotland = pharmacy_scotland.groupby(\"DispLocationPostcode\", as_index=False).agg({\n",
    "    \"DispLocationName\": lambda DispLocationName: \" | \".join(DispLocationName.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Merge with postcodes using Postcode and pcd7\n",
    "pharmacy_scotland = pharmacy_scotland.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\", \"geometry\"]],\n",
    "    left_on=\"DispLocationPostcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename columns to match standard format\n",
    "pharmacy_scotland = pharmacy_scotland.rename(columns={\n",
    "    \"DispLocationName\": \"Name\",\n",
    "    \"DispLocationPostcode\": \"Postcode\",\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "pharmacy_scotland = pharmacy_scotland[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\"]].copy()\n",
    "\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = pharmacy_scotland[pharmacy_scotland['Latitude'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = pharmacy_scotland.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            Latitude, Longitude = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                Latitude = data['result']['Latitude']\n",
    "                Longitude = data['result']['Longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                Latitude = data['terminated']['latitude']\n",
    "                Longitude = data['terminated']['longitude']\n",
    "            \n",
    "            if Latitude is not None and Longitude is not None:\n",
    "                pharmacy_scotland.at[idx, 'Latitude'] = Latitude\n",
    "                pharmacy_scotland.at[idx, 'Longitude'] = Longitude\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "pharmacy_scotland = gpd.GeoDataFrame(\n",
    "    pharmacy_scotland,\n",
    "    geometry=gpd.points_from_xy(pharmacy_scotland[\"Longitude\"], pharmacy_scotland[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f7e09b",
   "metadata": {},
   "source": [
    "### England and Wales\n",
    "The GP practice data for England and Wales is available from a direct download link (https://digital.nhs.uk/services/organisation-data-service/data-search-and-export/csv-downloads/gp-and-gp-practice-related-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a50b4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pharmacy data for England and Wales\n",
    "pharmacy_england_wales_url = \"https://www.odsdatasearchandexport.nhs.uk/api/getReport?report=edispensary\"\n",
    "pharmacy_england_wales = pd.read_csv(pharmacy_england_wales_url, header=None, low_memory=False, usecols=[1,9,12])\n",
    "pharmacy_england_wales.to_csv(\"./data/raw_data/pharmacy_england_wales_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "pharmacy_england_wales = pharmacy_england_wales[pharmacy_england_wales[12] == 'ACTIVE']\n",
    "\n",
    "pharmacy_england_wales.columns = ['Name', 'Postcode', 'Status']\n",
    "\n",
    "# Filter out rows where Postcode starts with 'JE' or 'IM' or 'GY' (Jersey / Isle of Man / Guernsey) and British Forces\n",
    "pharmacy_england_wales = pharmacy_england_wales[~pharmacy_england_wales['Postcode'].str.startswith(('JE', 'IM','GY','BF'))]\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "pharmacy_england_wales[\"Postcode\"] = pharmacy_england_wales[\"Postcode\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "\n",
    "pharmacy_england_wales = pharmacy_england_wales.drop(columns=[\"Status\"], errors=\"ignore\")\n",
    "\n",
    "pharmacy_england_wales = pharmacy_england_wales.groupby(\"Postcode\", as_index=False).agg({\n",
    "    \"Name\": lambda names: \" | \".join(names.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Merge with postcodes using Postcode and pcd7\n",
    "pharmacy_england_wales = pharmacy_england_wales.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\", \"geometry\"]],\n",
    "    left_on=\"Postcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rename columns to match standard format\n",
    "pharmacy_england_wales = pharmacy_england_wales.rename(columns={\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "pharmacy_england_wales = pharmacy_england_wales[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\"]].copy()\n",
    "\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = pharmacy_england_wales[pharmacy_england_wales['Latitude'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = pharmacy_england_wales.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            Latitude, Longitude = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                Latitude = data['result']['Latitude']\n",
    "                Longitude = data['result']['Longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                Latitude = data['terminated']['latitude']\n",
    "                Longitude = data['terminated']['longitude']\n",
    "            \n",
    "            if Latitude is not None and Longitude is not None:\n",
    "                pharmacy_england_wales.at[idx, 'Latitude'] = Latitude\n",
    "                pharmacy_england_wales.at[idx, 'Longitude'] = Longitude\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "pharmacy_england_wales = gpd.GeoDataFrame(\n",
    "    pharmacy_england_wales,\n",
    "    geometry=gpd.points_from_xy(pharmacy_england_wales[\"Longitude\"], pharmacy_england_wales[\"Latitude\"]),\n",
    "    crs=\"EPSG:4326\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "3d4d21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland pharmacies: 1297\n",
      "England and Wales pharmacies: 10899\n",
      "Saved pharmacies geoparquet to data/health/pharmacy.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine Pharmacy locations from England, Wales, and Scotland\n",
    "\n",
    "print(f\"Scotland pharmacies: {len(pharmacy_scotland)}\")\n",
    "print(f\"England and Wales pharmacies: {len(pharmacy_england_wales)}\")\n",
    "\n",
    "pharmacies_all = gpd.GeoDataFrame(\n",
    "    pd.concat([pharmacy_england_wales, pharmacy_scotland], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=postcodes.crs\n",
    ")\n",
    "\n",
    "health_dir = Path(\"data\") / \"health\"\n",
    "health_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export combined pharmacies as GeoParquet\n",
    "pharmacy_out_path = health_dir / \"pharmacy.parquet\"\n",
    "pharmacies_all.to_parquet(pharmacy_out_path, index=False)\n",
    "print(\"Saved pharmacies geoparquet to\", pharmacy_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7b31b",
   "metadata": {},
   "source": [
    "## Dentists\n",
    "### Scotland\n",
    "The dentist data for Scotland is available from a direct download link (https://www.opendata.nhs.scot/dataset/dental-practices-and-patient-registrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0f98b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dentist data for Scotland\n",
    "dentists_scotland_url = \"https://www.opendata.nhs.scot/dataset/2f218ba7-6695-4b22-867d-41383ae36de7/resource/a4eed9fe-bc2a-4a67-a2cd-ce0c765269b8/download/nhs-dental-practices-and-nhs-dental-registrations-as-at-30-jun-2025.csv\"\n",
    "dentists_scotland = pd.read_csv(dentists_scotland_url, low_memory=False)\n",
    "dentists_scotland.to_csv(\"./data/raw_data/dentists_scotland_20251001_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "# Select relevant columns (assuming 'PracticeName' and 'Postcode' based on typical structure)\n",
    "dentists_scotland = dentists_scotland[[\"address1\", \"pc7\"]]\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "dentists_scotland[\"pc7\"] = dentists_scotland[\"pc7\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "\n",
    "dentists_scotland = dentists_scotland[dentists_scotland['pc7'] != 'PA154LY'] # this had closed\n",
    "\n",
    "# Group by postcode to aggregate names\n",
    "dentists_scotland = dentists_scotland.groupby(\"pc7\", as_index=False).agg({\n",
    "    \"address1\": lambda address1: \" | \".join(address1.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "# Merge with postcodes using pc7 and pcd7\n",
    "dentists_scotland = dentists_scotland.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\", \"geometry\"]],\n",
    "    left_on=\"pc7\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = dentists_scotland[dentists_scotland['lat'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = dentists_scotland.at[idx, 'pc7']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            lat, long = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                lat = data['result']['latitude']\n",
    "                long = data['result']['longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                lat = data['terminated']['latitude']\n",
    "                long = data['terminated']['longitude']\n",
    "            \n",
    "            if lat is not None and long is not None:\n",
    "                dentists_scotland.at[idx, 'lat'] = lat\n",
    "                dentists_scotland.at[idx, 'long'] = long\n",
    "                dentists_scotland.at[idx, 'geometry'] = gpd.points_from_xy([long], [lat], crs=\"EPSG:4326\")[0]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "# Rename columns to match standard format\n",
    "dentists_scotland = dentists_scotland.rename(columns={\n",
    "    \"address1\": \"Name\",\n",
    "    \"pc7\": \"Postcode\",\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "dentists_scotland = dentists_scotland[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\", \"geometry\"]].copy()\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "dentists_scotland = gpd.GeoDataFrame(dentists_scotland, geometry=\"geometry\", crs=postcodes.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6aa6c",
   "metadata": {},
   "source": [
    "### England and Wales\n",
    "The GP practice data for England and Wales is available from a direct download link (https://digital.nhs.uk/services/organisation-data-service/data-search-and-export/csv-downloads/miscellaneous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "31b085fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dentist data for England and Wales\n",
    "dentists_england_wales_url = \"https://www.odsdatasearchandexport.nhs.uk/api/getReport?report=egdpprac\"\n",
    "\n",
    "dentists_england_wales = pd.read_csv(dentists_england_wales_url, header=None, low_memory=False)\n",
    "dentists_england_wales.to_csv(\"./data/raw_data/dentists_england_wales_raw.csv\", index=False) # Save a copy of the raw data\n",
    "\n",
    "#dentists_england_wales = pd.read_csv(dentists_england_wales_url, header=None, low_memory=False, usecols=[1,9,12])\n",
    "dentists_england_wales = dentists_england_wales[dentists_england_wales[12] == 'ACTIVE']\n",
    "dentists_england_wales = dentists_england_wales[[1, 9,4,5,7]]\n",
    "dentists_england_wales.columns = ['Name', 'Postcode', 'Address1', 'Address2', 'Address3']\n",
    "\n",
    "# Filter out rows where Postcode starts with 'JE' or 'IM' or 'GY' (Jersey / Isle of Man / Guernsey) and British Forces\n",
    "dentists_england_wales = dentists_england_wales[~dentists_england_wales['Postcode'].str.startswith(('JE', 'IM','GY','BF'))]\n",
    "\n",
    "# Clean postcodes by removing spaces and uppercasing\n",
    "dentists_england_wales[\"Postcode\"] = dentists_england_wales[\"Postcode\"].str.replace(\" \", \"\", regex=False).str.upper()\n",
    "\n",
    "# Merge with postcodes using Postcode and pcd7\n",
    "dentists_england_wales = dentists_england_wales.merge(\n",
    "    postcodes[[\"pcd7\", \"lat\", \"long\", \"geometry\"]],\n",
    "    left_on=\"Postcode\",\n",
    "    right_on=\"pcd7\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# For any missing lat/long, attempt to fetch from postcodes.io (these are all terminated postcodes which arent in the lookup)\n",
    "missing = dentists_england_wales[dentists_england_wales['lat'].isna()].copy()\n",
    "if not missing.empty:\n",
    "    for idx in missing.index:\n",
    "        postcode = dentists_england_wales.at[idx, 'Postcode']\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.postcodes.io/postcodes/{postcode}\")\n",
    "            data = response.json()\n",
    "            \n",
    "            lat, long = None, None\n",
    "            \n",
    "            # Active postcode\n",
    "            if data.get('status') == 200 and 'result' in data:\n",
    "                lat = data['result']['latitude']\n",
    "                long = data['result']['longitude']\n",
    "            # Terminated postcode - still has coordinates\n",
    "            elif 'terminated' in data:\n",
    "                lat = data['terminated']['latitude']\n",
    "                long = data['terminated']['longitude']\n",
    "            \n",
    "            if lat is not None and long is not None:\n",
    "                dentists_england_wales.at[idx, 'lat'] = lat\n",
    "                dentists_england_wales.at[idx, 'long'] = long\n",
    "                dentists_england_wales.at[idx, 'geometry'] = gpd.points_from_xy([long], [lat], crs=\"EPSG:4326\")[0]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data for postcode {postcode}: {e}\")\n",
    "\n",
    "\n",
    "# Rename columns to match standard format\n",
    "dentists_england_wales = dentists_england_wales.rename(columns={\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"long\": \"Longitude\"\n",
    "})\n",
    "\n",
    "# Select final columns\n",
    "dentists_england_wales = dentists_england_wales[[\"Name\", \"Latitude\", \"Longitude\", \"Postcode\", \"geometry\"]].copy()\n",
    "\n",
    "\n",
    "dentists_england_wales = dentists_england_wales.groupby([\"Postcode\", \"Latitude\", \"Longitude\",\"geometry\"], as_index=False).agg({\n",
    "    \"Name\": lambda Name: \" | \".join(Name.astype(str).str.strip().unique())\n",
    "})\n",
    "\n",
    "\n",
    "# Convert to GeoDataFrame\n",
    "dentists_england_wales = gpd.GeoDataFrame(dentists_england_wales, geometry=\"geometry\", crs=postcodes.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a69cedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland dentists: 940\n",
      "England and Wales dentists: 9106\n",
      "Saved dentists geoparquet to data/health/dentist.parquet\n"
     ]
    }
   ],
   "source": [
    "# Combine Dentist locations from England, Wales, and Scotland\n",
    "print(f\"Scotland dentists: {len(dentists_scotland)}\")\n",
    "print(f\"England and Wales dentists: {len(dentists_england_wales)}\")\n",
    "\n",
    "dentist_all = gpd.GeoDataFrame(\n",
    "    pd.concat([dentists_england_wales, dentists_scotland], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=postcodes.crs\n",
    ")\n",
    "\n",
    "health_dir = Path(\"data\") / \"health\"\n",
    "health_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export combined dentists as GeoParquet\n",
    "dentist_out_path = health_dir / \"dentist.parquet\"\n",
    "dentist_all.to_parquet(dentist_out_path, index=False)\n",
    "print(\"Saved dentists geoparquet to\", dentist_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1821d327",
   "metadata": {},
   "source": [
    "# Air Quality\n",
    "\n",
    "Gridded air quality data are all sourced directly from - https://uk-air.defra.gov.uk/data/pcm-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9b65884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PM10 geoparquet to data/raw_data/PM10.parquet\n",
      "Saved NO2 geoparquet to data/raw_data/NO2.parquet\n",
      "Saved SO2 geoparquet to data/raw_data/SO2.parquet\n"
     ]
    }
   ],
   "source": [
    "PM10 = pd.read_csv('https://uk-air.defra.gov.uk/datastore/pcm/mappm102024g.csv', skiprows=5, na_values=['MISSING']) # one values is 'MISSING'\n",
    "NO2 = pd.read_csv('https://uk-air.defra.gov.uk/datastore/pcm/mapnox2024.csv', skiprows=5)\n",
    "SO2 = pd.read_csv('https://uk-air.defra.gov.uk/datastore/pcm/mapso22024.csv', skiprows=5)\n",
    "\n",
    "out_dir = Path(\"data\") / \"raw_data\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PM10.to_csv(out_dir / \"PM10_2024.csv\", index=False) # Save PM10 data\n",
    "NO2.to_csv(out_dir / \"NO2_2024.csv\", index=False) # Save NO2 data\n",
    "SO2.to_csv(out_dir / \"SO2_2024.csv\", index=False) # Save SO2 data\n",
    "\n",
    "PM10_gdf = gpd.GeoDataFrame(PM10, geometry=gpd.points_from_xy(PM10['x'], PM10['y']), crs='EPSG:27700')\n",
    "NO2_gdf = gpd.GeoDataFrame(NO2, geometry=gpd.points_from_xy(NO2['x'], NO2['y']), crs='EPSG:27700')\n",
    "SO2_gdf = gpd.GeoDataFrame(SO2, geometry=gpd.points_from_xy(SO2['x'], SO2['y']), crs='EPSG:27700')\n",
    "\n",
    "out_dir = Path(\"data\") / \"raw_data\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pm10_out_path = out_dir / \"PM10.parquet\"\n",
    "PM10_gdf.to_parquet(pm10_out_path, index=False)\n",
    "print(\"Saved PM10 geoparquet to\", pm10_out_path)\n",
    "\n",
    "no2_out_path = out_dir / \"NO2.parquet\"\n",
    "NO2_gdf.to_parquet(no2_out_path, index=False)\n",
    "print(\"Saved NO2 geoparquet to\", no2_out_path)\n",
    "\n",
    "so2_out_path = out_dir / \"SO2.parquet\"\n",
    "SO2_gdf.to_parquet(so2_out_path, index=False)\n",
    "print(\"Saved SO2 geoparquet to\", so2_out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5b6d42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate area-weighted average of gridded point data for polygons\n",
    "\n",
    "def area_weighted_average(\n",
    "    points_gdf: gpd.GeoDataFrame,\n",
    "    polygons_gdf: gpd.GeoDataFrame,\n",
    "    value_col: str,\n",
    "    grid_size: float = 1000,\n",
    "    polygon_id_col: str = None\n",
    ") -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculate area-weighted average of gridded point data for each polygon.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    points_gdf : GeoDataFrame\n",
    "        Point centroids of grid cells with values to aggregate\n",
    "    polygons_gdf : GeoDataFrame\n",
    "        Polygon boundaries to aggregate into\n",
    "    value_col : str\n",
    "        Column name in points_gdf containing values to average\n",
    "    grid_size : float\n",
    "        Size of grid cells in CRS units (default 1000m for 1km grid)\n",
    "    polygon_id_col : str, optional\n",
    "        Column to use as polygon identifier. If None, uses index.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        Original polygons with new column '{value_col}_weighted_mean'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Work with copies to avoid modifying originals\n",
    "    points = points_gdf.copy()\n",
    "    polygons = polygons_gdf.copy()\n",
    "    \n",
    "    # Ensure same CRS\n",
    "    if points.crs != polygons.crs:\n",
    "        points = points.to_crs(polygons.crs)\n",
    "    \n",
    "    # Create polygon ID if not specified\n",
    "    if polygon_id_col is None:\n",
    "        polygons['_poly_id'] = polygons.index\n",
    "        polygon_id_col = '_poly_id'\n",
    "    \n",
    "    # Convert point centroids to grid squares\n",
    "    half_grid = grid_size / 2\n",
    "    points['geometry'] = points.geometry.buffer(half_grid, cap_style=3)\n",
    "    \n",
    "    # Keep only necessary columns for overlay\n",
    "    points_subset = points[[value_col, 'geometry']].copy()\n",
    "    polygons_subset = polygons[[polygon_id_col, 'geometry']].copy()\n",
    "    \n",
    "    # Intersect grids with boundaries\n",
    "    intersected = gpd.overlay(points_subset, polygons_subset, how='intersection')\n",
    "    \n",
    "    # Calculate intersection areas\n",
    "    intersected['_intersect_area'] = intersected.geometry.area\n",
    "    \n",
    "    # Calculate weighted values\n",
    "    intersected['_weighted_value'] = intersected[value_col] * intersected['_intersect_area']\n",
    "    \n",
    "    # Aggregate by polygon\n",
    "    agg = intersected.groupby(polygon_id_col).agg({\n",
    "        '_weighted_value': 'sum',\n",
    "        '_intersect_area': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Calculate weighted mean\n",
    "    result_col = f'{value_col}_weighted_mean'\n",
    "    agg[result_col] = agg['_weighted_value'] / agg['_intersect_area']\n",
    "    \n",
    "    # Merge back to original polygons\n",
    "    result = polygons.merge(\n",
    "        agg[[polygon_id_col, result_col]], \n",
    "        on=polygon_id_col, \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Clean up temp columns\n",
    "    if '_poly_id' in result.columns:\n",
    "        result = result.drop(columns=['_poly_id'])\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ee7b2332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/geopandas/tools/overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 467 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n",
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/geopandas/tools/overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 467 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n",
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/geopandas/tools/overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 467 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    }
   ],
   "source": [
    "# Calculate area-weighted averages for each pollutant\n",
    "# PM10\n",
    "boundaries_with_pm10 = area_weighted_average(\n",
    "    points_gdf=PM10_gdf,\n",
    "    polygons_gdf=boundaries_all,\n",
    "    value_col='pm102024g',\n",
    "    grid_size=1000\n",
    ")\n",
    "\n",
    "# NO2\n",
    "boundaries_with_no2 = area_weighted_average(\n",
    "    points_gdf=NO2_gdf,\n",
    "    polygons_gdf=boundaries_all,\n",
    "    value_col='nox2024',\n",
    "    grid_size=1000\n",
    ")\n",
    "\n",
    "# SO2\n",
    "boundaries_with_so2 = area_weighted_average(\n",
    "    points_gdf=SO2_gdf,\n",
    "    polygons_gdf=boundaries_all,\n",
    "    value_col='so22024',\n",
    "    grid_size=1000\n",
    ")\n",
    "\n",
    "out_dir = Path(\"data\") / \"airquality\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pm10_out_path = out_dir / \"pm10.parquet\"\n",
    "boundaries_with_pm10.to_parquet(pm10_out_path, index=False)\n",
    "print(\"Saved boundaries_with_pm10 geoparquet to\", pm10_out_path)\n",
    "\n",
    "no2_out_path = out_dir / \"no2.parquet\"\n",
    "boundaries_with_no2.to_parquet(no2_out_path, index=False)\n",
    "print(\"Saved boundaries_with_no2 geoparquet to\", no2_out_path)\n",
    "\n",
    "so2_out_path = out_dir / \"so2.parquet\"\n",
    "boundaries_with_so2.to_parquet(so2_out_path, index=False)\n",
    "print(\"Saved boundaries_with_so2 geoparquet to\", so2_out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736c3a5",
   "metadata": {},
   "source": [
    "# Green / Blue Space\n",
    "## Blue Space\n",
    "\n",
    "The following code requires Osmium - https://osmcode.org/osmium-tool/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "705d9660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('data/raw_data/great-britain-latest.osm.pbf'),\n",
       " <http.client.HTTPMessage at 0x51625e4b0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Great Britain OSM PBF file\n",
    "url = \"https://download.geofabrik.de/europe/great-britain-latest.osm.pbf\"\n",
    "out_dir = Path(\"data\") / \"raw_data\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"great-britain-latest.osm.pbf\"\n",
    "urllib.request.urlretrieve(url, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d1561e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_database: /opt/anaconda3/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 2 whereas a number >= 5 is expected. It comes from another PROJ installation.\n",
      "Warning 1: Field 'area' already exists. Renaming it as 'area2'\n",
      "Warning 1: Field 'fixme' already exists. Renaming it as 'fixme2'\n",
      "Warning 1: Field 'notes' already exists. Renaming it as 'notes2'\n",
      "Warning 1: Field 'todo' already exists. Renaming it as 'todo2'\n",
      "ERROR 1: PROJ: proj_create_from_database: /opt/anaconda3/share/proj/proj.db contains DATABASE.LAYOUT.VERSION.MINOR = 2 whereas a number >= 5 is expected. It comes from another PROJ installation.\n",
      "Warning 1: Field 'fixme' already exists. Renaming it as 'fixme2'\n"
     ]
    }
   ],
   "source": [
    "# Filter for water features\n",
    "subprocess.run([\n",
    "    \"osmium\", \"tags-filter\", \"data/raw_data/great-britain-latest.osm.pbf\",\n",
    "    \"nwr/natural=water\", \"w/waterway=*\", \"-o\", \"data/raw_data/gb-water.osm.pbf\"\n",
    "], check=True)\n",
    "\n",
    "# Export to GeoJSON\n",
    "subprocess.run([\n",
    "    \"osmium\", \"export\", \"data/raw_data/gb-water.osm.pbf\",\n",
    "    \"-o\", \"data/raw_data/gb-water.geojson\"\n",
    "], check=True)\n",
    "\n",
    "# Convert to Parquet\n",
    "subprocess.run([\n",
    "    \"ogr2ogr\", \"-f\", \"Parquet\", \"data/raw_data/gb-water.parquet\",\n",
    "    \"data/raw_data/gb-water.geojson\"\n",
    "], check=True)\n",
    "\n",
    "# Filter for coastline\n",
    "subprocess.run([\n",
    "    \"osmium\", \"tags-filter\", \"data/raw_data/great-britain-latest.osm.pbf\",\n",
    "    \"w/natural=coastline\", \"-o\", \"data/raw_data/gb-coast.osm.pbf\"\n",
    "], check=True)\n",
    "\n",
    "# Export to GeoJSON\n",
    "subprocess.run([\n",
    "    \"osmium\", \"export\", \"data/raw_data/gb-coast.osm.pbf\",\n",
    "    \"-o\", \"data/raw_data/gb-coast.geojson\"\n",
    "], check=True)\n",
    "\n",
    "# Convert to Parquet\n",
    "subprocess.run([\n",
    "    \"ogr2ogr\", \"-f\", \"Parquet\", \"data/raw_data/gb-coast.parquet\",\n",
    "    \"data/raw_data/gb-coast.geojson\"\n",
    "], check=True)\n",
    "\n",
    "# Delete the GeoJSON / PBF files after conversion\n",
    "os.remove(\"data/raw_data/gb-water.geojson\")\n",
    "os.remove(\"data/raw_data/gb-coast.geojson\")\n",
    "\n",
    "os.remove(\"data/raw_data/gb-water.osm.pbf\")\n",
    "os.remove(\"data/raw_data/gb-coast.osm.pbf\")\n",
    "\n",
    "out_path.unlink() # remove the original pbf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e237ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the coastline and larger water bodies\n",
    "gb_coast = gpd.read_parquet(\"data/raw_data/gb-coast.parquet\")\n",
    "gb_coast = gb_coast[gb_coast.geometry.type == 'LineString']\n",
    "gb_water = gpd.read_parquet(\"data/raw_data/gb-water.parquet\")\n",
    "gb_water = gb_water[gb_water.geometry.type.isin(['Polygon', 'MultiPolygon'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5e4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to OSGB (British National Grid) and calculate areas\n",
    "gb_water = gb_water.to_crs('EPSG:27700')\n",
    "\n",
    "# Calculate areas for water polygons\n",
    "gb_water['area_m2'] = gb_water.geometry.area\n",
    "gb_water = gb_water[gb_water['area_m2'] >= 7500].copy()\n",
    "gb_water = gb_water.to_crs(gb_coast.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39025878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 49.9 MB\n",
      "Loaded 192,738 river features\n",
      "Geometry types: ['LineString']\n"
     ]
    }
   ],
   "source": [
    "# Import OS Rivers Data\n",
    "\n",
    "# Download OS Open Rivers data\n",
    "rivers_url = \"https://api.os.uk/downloads/v1/products/OpenRivers/downloads?area=GB&format=GeoPackage&redirect\"\n",
    "\n",
    "rivers_tmp_dir = tempfile.mkdtemp()\n",
    "rivers_zip_path = os.path.join(rivers_tmp_dir, \"oprvrs_gpkg_gb.zip\")\n",
    "\n",
    "response = requests.get(rivers_url, allow_redirects=True, timeout=120)\n",
    "response.raise_for_status()\n",
    "\n",
    "print(f\"Downloaded {len(response.content) / 1024**2:.1f} MB\")\n",
    "\n",
    "with open(rivers_zip_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract\n",
    "with zipfile.ZipFile(rivers_zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(rivers_tmp_dir)\n",
    "\n",
    "rivers_gpkg_path = os.path.join(rivers_tmp_dir, \"Data\", \"oprvrs_gb.gpkg\")\n",
    "\n",
    "# Read the watercourse_link layer which contains LineStrings (not the default hydro_node layer)\n",
    "gb_rivers = gpd.read_file(rivers_gpkg_path, layer='watercourse_link')\n",
    "print(f\"Loaded {len(gb_rivers):,} river features\")\n",
    "\n",
    "# Verify we have LineStrings\n",
    "print(f\"Geometry types: {gb_rivers.geometry.type.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25c57b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flow_direction</th>\n",
       "      <th>length</th>\n",
       "      <th>fictitious</th>\n",
       "      <th>form</th>\n",
       "      <th>watercourse_name</th>\n",
       "      <th>watercourse_name_alternative</th>\n",
       "      <th>start_node</th>\n",
       "      <th>end_node</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73E1A217-C5BF-41E3-AA17-F969F5181E04</td>\n",
       "      <td>in direction</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0</td>\n",
       "      <td>tidalRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>6B77D6D2-B9B2-4FBE-AEA3-8A801079B231</td>\n",
       "      <td>35806C2B-E01E-4CB8-9767-3D6F86302713</td>\n",
       "      <td>LINESTRING (-0.83541 60.7855, -0.83519 60.78562)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A44E2603-3307-4555-9604-3DAB8AA308C6</td>\n",
       "      <td>in direction</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8FA84F8B-71CE-4626-8CEC-5964B94D34CC</td>\n",
       "      <td>D2571525-C2FD-4A2A-8236-7E938081B3E7</td>\n",
       "      <td>LINESTRING (-0.83421 60.79485, -0.8334 60.7948...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83332CB1-EC9C-486C-AC88-0BE480176548</td>\n",
       "      <td>in direction</td>\n",
       "      <td>1352.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>Burn of Norwick</td>\n",
       "      <td>None</td>\n",
       "      <td>5EEC6ADE-DF7B-4B72-BE47-C78F810C39BF</td>\n",
       "      <td>C3EE6BFE-7B45-4251-951D-4F2A3BB4BBC3</td>\n",
       "      <td>LINESTRING (-0.82643 60.80476, -0.82665 60.804...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83C8021E-C7EF-4C2B-955A-3365D9DDF079</td>\n",
       "      <td>in direction</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>5CF9C9E6-F0D3-4421-81AD-989455DD490C</td>\n",
       "      <td>5EEC6ADE-DF7B-4B72-BE47-C78F810C39BF</td>\n",
       "      <td>LINESTRING (-0.82584 60.80594, -0.82555 60.805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58F9A511-A4BD-47F4-95C1-FE396F677E1B</td>\n",
       "      <td>in direction</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>D2571525-C2FD-4A2A-8236-7E938081B3E7</td>\n",
       "      <td>1740F423-BAF7-4B61-AE72-743CC7BE3D60</td>\n",
       "      <td>LINESTRING (-0.82285 60.78971, -0.82286 60.78947)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192733</th>\n",
       "      <td>7EB00B29-3123-49FD-AD44-8340BB33F404</td>\n",
       "      <td>in direction</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>04E1EF1B-618C-4BBF-A345-5C783A897E02</td>\n",
       "      <td>5EE99987-0230-4BBB-BDFC-70C96B139C28</td>\n",
       "      <td>LINESTRING (-5.61168 50.14567, -5.6119 50.1455...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192734</th>\n",
       "      <td>3355B2D8-576B-430D-BE7D-382B926F0FE7</td>\n",
       "      <td>in direction</td>\n",
       "      <td>732.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>937C50B0-E3C8-4006-8C05-299E0D3B7334</td>\n",
       "      <td>1FEFFD80-26F0-4BB0-880A-63DE127AD449</td>\n",
       "      <td>LINESTRING (-5.56626 50.16696, -5.56618 50.166...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192735</th>\n",
       "      <td>EE3F229D-62CF-4877-B57F-ADE942804371</td>\n",
       "      <td>in direction</td>\n",
       "      <td>568.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>03D09FEA-9748-456F-9B92-F0B657624E1C</td>\n",
       "      <td>56037488-0E8B-4A60-8934-7EE1339012EB</td>\n",
       "      <td>LINESTRING (-5.65054 50.16165, -5.65119 50.162...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192736</th>\n",
       "      <td>AC23A1FC-7C84-48AF-9E49-EDD08442AAE1</td>\n",
       "      <td>in direction</td>\n",
       "      <td>733.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2F32710C-3CF5-4EEF-B5F8-7D4CFAE7429B</td>\n",
       "      <td>3C77BFD9-D00F-40A5-9D5B-D916B99638CD</td>\n",
       "      <td>LINESTRING (-5.60002 50.17741, -5.60035 50.177...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192737</th>\n",
       "      <td>9A961B70-B8D7-49F8-B829-378EC974F981</td>\n",
       "      <td>in direction</td>\n",
       "      <td>552.0</td>\n",
       "      <td>0</td>\n",
       "      <td>inlandRiver</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>647EC346-2DF4-47D3-9F8D-F76F0B50B93F</td>\n",
       "      <td>D3219C55-363C-4A3A-9580-E734B322891C</td>\n",
       "      <td>LINESTRING (-5.53961 50.20752, -5.54055 50.208...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192738 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          id flow_direction  length  \\\n",
       "0       73E1A217-C5BF-41E3-AA17-F969F5181E04   in direction    18.0   \n",
       "1       A44E2603-3307-4555-9604-3DAB8AA308C6   in direction  1009.0   \n",
       "2       83332CB1-EC9C-486C-AC88-0BE480176548   in direction  1352.0   \n",
       "3       83C8021E-C7EF-4C2B-955A-3365D9DDF079   in direction   165.0   \n",
       "4       58F9A511-A4BD-47F4-95C1-FE396F677E1B   in direction    27.0   \n",
       "...                                      ...            ...     ...   \n",
       "192733  7EB00B29-3123-49FD-AD44-8340BB33F404   in direction   232.0   \n",
       "192734  3355B2D8-576B-430D-BE7D-382B926F0FE7   in direction   732.0   \n",
       "192735  EE3F229D-62CF-4877-B57F-ADE942804371   in direction   568.0   \n",
       "192736  AC23A1FC-7C84-48AF-9E49-EDD08442AAE1   in direction   733.0   \n",
       "192737  9A961B70-B8D7-49F8-B829-378EC974F981   in direction   552.0   \n",
       "\n",
       "       fictitious         form watercourse_name watercourse_name_alternative  \\\n",
       "0               0   tidalRiver             None                         None   \n",
       "1               0  inlandRiver             None                         None   \n",
       "2               0  inlandRiver  Burn of Norwick                         None   \n",
       "3               0  inlandRiver             None                         None   \n",
       "4               0  inlandRiver             None                         None   \n",
       "...           ...          ...              ...                          ...   \n",
       "192733          0  inlandRiver             None                         None   \n",
       "192734          0  inlandRiver             None                         None   \n",
       "192735          0  inlandRiver             None                         None   \n",
       "192736          0  inlandRiver             None                         None   \n",
       "192737          0  inlandRiver             None                         None   \n",
       "\n",
       "                                  start_node  \\\n",
       "0       6B77D6D2-B9B2-4FBE-AEA3-8A801079B231   \n",
       "1       8FA84F8B-71CE-4626-8CEC-5964B94D34CC   \n",
       "2       5EEC6ADE-DF7B-4B72-BE47-C78F810C39BF   \n",
       "3       5CF9C9E6-F0D3-4421-81AD-989455DD490C   \n",
       "4       D2571525-C2FD-4A2A-8236-7E938081B3E7   \n",
       "...                                      ...   \n",
       "192733  04E1EF1B-618C-4BBF-A345-5C783A897E02   \n",
       "192734  937C50B0-E3C8-4006-8C05-299E0D3B7334   \n",
       "192735  03D09FEA-9748-456F-9B92-F0B657624E1C   \n",
       "192736  2F32710C-3CF5-4EEF-B5F8-7D4CFAE7429B   \n",
       "192737  647EC346-2DF4-47D3-9F8D-F76F0B50B93F   \n",
       "\n",
       "                                    end_node  \\\n",
       "0       35806C2B-E01E-4CB8-9767-3D6F86302713   \n",
       "1       D2571525-C2FD-4A2A-8236-7E938081B3E7   \n",
       "2       C3EE6BFE-7B45-4251-951D-4F2A3BB4BBC3   \n",
       "3       5EEC6ADE-DF7B-4B72-BE47-C78F810C39BF   \n",
       "4       1740F423-BAF7-4B61-AE72-743CC7BE3D60   \n",
       "...                                      ...   \n",
       "192733  5EE99987-0230-4BBB-BDFC-70C96B139C28   \n",
       "192734  1FEFFD80-26F0-4BB0-880A-63DE127AD449   \n",
       "192735  56037488-0E8B-4A60-8934-7EE1339012EB   \n",
       "192736  3C77BFD9-D00F-40A5-9D5B-D916B99638CD   \n",
       "192737  D3219C55-363C-4A3A-9580-E734B322891C   \n",
       "\n",
       "                                                 geometry  \n",
       "0        LINESTRING (-0.83541 60.7855, -0.83519 60.78562)  \n",
       "1       LINESTRING (-0.83421 60.79485, -0.8334 60.7948...  \n",
       "2       LINESTRING (-0.82643 60.80476, -0.82665 60.804...  \n",
       "3       LINESTRING (-0.82584 60.80594, -0.82555 60.805...  \n",
       "4       LINESTRING (-0.82285 60.78971, -0.82286 60.78947)  \n",
       "...                                                   ...  \n",
       "192733  LINESTRING (-5.61168 50.14567, -5.6119 50.1455...  \n",
       "192734  LINESTRING (-5.56626 50.16696, -5.56618 50.166...  \n",
       "192735  LINESTRING (-5.65054 50.16165, -5.65119 50.162...  \n",
       "192736  LINESTRING (-5.60002 50.17741, -5.60035 50.177...  \n",
       "192737  LINESTRING (-5.53961 50.20752, -5.54055 50.208...  \n",
       "\n",
       "[192738 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter gb_rivers to keep only LineString geometries\n",
    "gb_rivers = gb_rivers.to_crs(gb_coast.crs)\n",
    "gb_rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1574fccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined blue space points to data/green_blue/bluespace.parquet\n",
      "Total points: 8,574,806\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Convert LineStrings to points\n",
    "from shapely import get_coordinates\n",
    "\n",
    "coords = get_coordinates(gb_water.geometry)\n",
    "gb_water_points = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(coords[:, 0], coords[:, 1]),\n",
    "    crs=gb_water.crs\n",
    ")\n",
    "\n",
    "coords = get_coordinates(gb_coast.geometry)\n",
    "gb_coast_points = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(coords[:, 0], coords[:, 1]),\n",
    "    crs=gb_coast.crs\n",
    ")\n",
    "\n",
    "coords = get_coordinates(gb_rivers.geometry)\n",
    "gb_rivers_points = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(coords[:, 0], coords[:, 1]),\n",
    "    crs=gb_rivers.crs\n",
    ")\n",
    "\n",
    "# Combine gb_water_points, gb_coast_points, and gb_rivers_points\n",
    "gb_blue_space = gpd.GeoDataFrame(\n",
    "    pd.concat([gb_water_points, gb_coast_points, gb_rivers_points], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=gb_water_points.crs\n",
    ")\n",
    "\n",
    "# Save to parquet\n",
    "blue_space_dir = Path(\"data\") / \"green_blue\"\n",
    "blue_space_dir.mkdir(parents=True, exist_ok=True)\n",
    "blue_space_out_path = blue_space_dir / \"bluespace.parquet\"\n",
    "gb_blue_space.to_parquet(blue_space_out_path, index=False)\n",
    "print(f\"Saved combined blue space points to {blue_space_out_path}\")\n",
    "print(f\"Total points: {len(gb_blue_space):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9e84e8",
   "metadata": {},
   "source": [
    "# Greenspace\n",
    "## Ambient Greenspace\n",
    "\n",
    "The following code generates the ambient Greenspace indicator.\n",
    "\n",
    "### Google Cloud Setup\n",
    "\n",
    "There are a few steps to set up Google Cloud for use with Earth Engine and Cloud Storage.\n",
    "\n",
    "1. **Install Google Cloud SDK**\n",
    "For example - on macOS with MacPorts\n",
    "\n",
    "```sudo port install google-cloud-sdk```\n",
    "\n",
    "2. **Authenticate with Google Cloud**\n",
    "The following command needs to be run in a terminal and will open a browser window for authentication\n",
    "\n",
    "```bashgcloud auth login```\n",
    "\n",
    "3. **Create a Google Cloud Project**\n",
    "\n",
    "Go to [console.cloud.google.com](https://console.cloud.google.com/)\n",
    "Create a new project or select an existing one\n",
    "Note your project ID\n",
    "\n",
    "4. **Enable Required APIs**\n",
    "\n",
    "Enable the Earth Engine API in the project\n",
    "Enable the Cloud Storage API in the project\n",
    "\n",
    "5. **Register for Earth Engine**\n",
    "\n",
    "Visit code.earthengine.google.com and register your Google account\n",
    "\n",
    "6. **Create a Cloud Storage Bucket**\n",
    "\n",
    "In Cloud Console, go to Cloud Storage and \"Create bucket\"\n",
    "The following code expects a bucket named 'a-h-a-h'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5d1ef",
   "metadata": {},
   "source": [
    "### Create a UK Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc4e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/2v6lbkmd1s3_lyxghzghl0rm0000gn/T/ipykernel_12695/1023555476.py:36: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  boundaries_gee.to_file(shp_path, driver='ESRI Shapefile')\n",
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/pyogrio/raw.py:733: RuntimeWarning: Normalized/laundered field name: 'LSOA_DZ_SDZ_21_22' to 'LSOA_DZ_SD'\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/raw_data/gee_upload/boundaries_uk.shp\n"
     ]
    }
   ],
   "source": [
    "boundaries_all_UK = gpd.read_parquet(Path(\"data\") / \"boundary\" / \"LSOA_DZ_SDZ_21_22.parquet\")\n",
    "\n",
    "zip_url = \"https://www.nisra.gov.uk/files/nisra/publications/geography-sdz2021-esri-shapefile.zip\"\n",
    "\n",
    "# create a temporary directory and download the zip there\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "zip_path = os.path.join(tmp_dir, \"sdz2021.zip\")\n",
    "urllib.request.urlretrieve(zip_url, zip_path)\n",
    "\n",
    "# Extract and import Northern Ireland SDZ 2021 shapefile\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(tmp_dir)\n",
    "\n",
    "# Read the shapefile\n",
    "NI_boundaries = gpd.read_file(os.path.join(tmp_dir, \"SDZ2021.shp\"))\n",
    "\n",
    "NI_boundaries = NI_boundaries.rename(columns={\"SDZ2021_cd\": \"LSOA_DZ_SDZ_21_22\"})\n",
    "NI_boundaries = NI_boundaries[[\"LSOA_DZ_SDZ_21_22\", \"geometry\"]].copy()\n",
    "\n",
    "NI_boundaries = NI_boundaries.to_crs(boundaries_all.crs)\n",
    "\n",
    "boundaries_all_UK = gpd.GeoDataFrame(\n",
    "    pd.concat([boundaries_all, NI_boundaries], ignore_index=True),\n",
    "    geometry=\"geometry\",\n",
    "    crs=boundaries_all.crs\n",
    ")\n",
    "\n",
    "# --- Reproject to WGS84 (required for GEE) ---\n",
    "boundaries_gee = boundaries_all_UK.to_crs('EPSG:4326')\n",
    "\n",
    "# Export to shapefile\n",
    "output_dir = Path('./data/raw_data/gee_upload')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "shp_path = output_dir / 'boundaries_uk.shp'\n",
    "boundaries_gee.to_file(shp_path, driver='ESRI Shapefile')\n",
    "\n",
    "print(f'Saved to {shp_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6c5f5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a17cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Earth Engine with a specific project\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project='ndvi-inspire')\n",
    "\n",
    "# Set the project\n",
    "result = subprocess.run(['earthengine', 'set_project', 'ndvi-inspire'], capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f818e2",
   "metadata": {},
   "source": [
    "#### Upload Shapefile to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba3051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reprojected from {\"$schema\": \"https://proj.org/schemas/v0.7/projjson.schema.json\", \"type\": \"ProjectedCRS\", \"name\": \"OSGB36 / British National Grid\", \"base_crs\": {\"name\": \"OSGB36\", \"datum\": {\"type\": \"GeodeticReferenceFrame\", \"name\": \"Ordnance Survey of Great Britain 1936\", \"ellipsoid\": {\"name\": \"Airy 1830\", \"semi_major_axis\": 6377563.396, \"inverse_flattening\": 299.3249646}}, \"coordinate_system\": {\"subtype\": \"ellipsoidal\", \"axis\": [{\"name\": \"Geodetic latitude\", \"abbreviation\": \"Lat\", \"direction\": \"north\", \"unit\": \"degree\"}, {\"name\": \"Geodetic longitude\", \"abbreviation\": \"Lon\", \"direction\": \"east\", \"unit\": \"degree\"}]}, \"id\": {\"authority\": \"EPSG\", \"code\": 4277}}, \"conversion\": {\"name\": \"British National Grid\", \"method\": {\"name\": \"Transverse Mercator\", \"id\": {\"authority\": \"EPSG\", \"code\": 9807}}, \"parameters\": [{\"name\": \"Latitude of natural origin\", \"value\": 49, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8801}}, {\"name\": \"Longitude of natural origin\", \"value\": -2, \"unit\": \"degree\", \"id\": {\"authority\": \"EPSG\", \"code\": 8802}}, {\"name\": \"Scale factor at natural origin\", \"value\": 0.9996012717, \"unit\": \"unity\", \"id\": {\"authority\": \"EPSG\", \"code\": 8805}}, {\"name\": \"False easting\", \"value\": 400000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8806}}, {\"name\": \"False northing\", \"value\": -100000, \"unit\": \"metre\", \"id\": {\"authority\": \"EPSG\", \"code\": 8807}}]}, \"coordinate_system\": {\"subtype\": \"Cartesian\", \"axis\": [{\"name\": \"Easting\", \"abbreviation\": \"E\", \"direction\": \"east\", \"unit\": \"metre\"}, {\"name\": \"Northing\", \"abbreviation\": \"N\", \"direction\": \"north\", \"unit\": \"metre\"}]}, \"scope\": \"Engineering survey, topographic mapping.\", \"area\": \"United Kingdom (UK) - offshore to boundary of UKCS within 49\\u00b045'N to 61\\u00b0N and 9\\u00b0W to 2\\u00b0E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.\", \"bbox\": {\"south_latitude\": 49.75, \"west_longitude\": -9.01, \"north_latitude\": 61.01, \"east_longitude\": 2.01}, \"id\": {\"authority\": \"EPSG\", \"code\": 27700}} to EPSG:4326\n",
      "Features: 43,914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cr/2v6lbkmd1s3_lyxghzghl0rm0000gn/T/ipykernel_12695/2701278731.py:15: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  boundaries_gee.to_file(shp_path, driver='ESRI Shapefile')\n",
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/pyogrio/raw.py:733: RuntimeWarning: Normalized/laundered field name: 'LSOA_DZ_SDZ_21_22' to 'LSOA_DZ_SD'\n",
      "  ogr_write(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to gee_upload/boundaries_uk.shp\n",
      "Uploading boundaries_uk.shp...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://gee_upload/boundaries_uk.shp [Content-Type=application/octet-stream]...\n",
      "- [1 files][112.2 MiB/112.2 MiB]                                                \n",
      "Operation completed over 1 objects/112.2 MiB.                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading boundaries_uk.shx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://gee_upload/boundaries_uk.shx [Content-Type=application/octet-stream]...\n",
      "/ [1 files][343.2 KiB/343.2 KiB]                                                \n",
      "Operation completed over 1 objects/343.2 KiB.                                    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading boundaries_uk.dbf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://gee_upload/boundaries_uk.dbf [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  3.4 MiB/  3.4 MiB]                                                \n",
      "Operation completed over 1 objects/3.4 MiB.                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading boundaries_uk.prj...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://gee_upload/boundaries_uk.prj [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  145.0 B/  145.0 B]                                                \n",
      "Operation completed over 1 objects/145.0 B.                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading boundaries_uk.cpg...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://gee_upload/boundaries_uk.cpg [Content-Type=application/octet-stream]...\n",
      "/ [1 files][    5.0 B/    5.0 B]                                                \n",
      "Operation completed over 1 objects/5.0 B.                                        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files uploaded to GCS\n",
      "Running: earthengine upload table --asset_id=projects/ndvi-inspire/assets/boundaries_uk gs://a-h-a-h/gee_uploads/boundaries_uk.shp\n",
      "stdout: Started upload task with ID: A3WF3I3BUPXU5YKLUU7JEK5A\n",
      "\n",
      "stderr: \n"
     ]
    }
   ],
   "source": [
    "# Upload all shapefile components to GCS ---\n",
    "bucket = 'a-h-a-h'\n",
    "gcs_folder = 'gee_uploads'\n",
    "\n",
    "shp_extensions = ['.shp', '.shx', '.dbf', '.prj', '.cpg']\n",
    "\n",
    "for ext in shp_extensions:\n",
    "    local_file = shp_path.with_suffix(ext)\n",
    "    if local_file.exists():\n",
    "        gcs_path = f'gs://{bucket}/{gcs_folder}/{local_file.name}'\n",
    "        print(f'Uploading {local_file.name}...')\n",
    "        subprocess.run(['gsutil', 'cp', str(local_file), gcs_path], check=True)\n",
    "\n",
    "print('All files uploaded to GCS')\n",
    "\n",
    "# --- Step 4: Ingest into Earth Engine ---\n",
    "asset_id = 'projects/ndvi-inspire/assets/boundaries_uk'\n",
    "gcs_shp = f'gs://{bucket}/{gcs_folder}/boundaries_uk.shp'\n",
    "\n",
    "cmd = [\n",
    "    'earthengine',\n",
    "    'upload',\n",
    "    'table',\n",
    "    f'--asset_id={asset_id}',\n",
    "    gcs_shp\n",
    "]\n",
    "\n",
    "print(f'Running: {\" \".join(cmd)}')\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "\n",
    "print('stdout:', result.stdout)\n",
    "print('stderr:', result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd9e9a2",
   "metadata": {},
   "source": [
    "### Extract Greenspace within LSOAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9335ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud masking for Sentinel-2\n",
    "def mask_s2_clouds(image):\n",
    "    \"\"\"Mask clouds using SCL band, but keep original for pixel counting\"\"\"\n",
    "    scl = image.select('SCL')\n",
    "    mask = scl.neq(3).And(scl.neq(8)).And(scl.neq(9)).And(scl.neq(10))\n",
    "    \n",
    "    total_pixels = ee.Image.constant(1).rename('total_pixels')\n",
    "    valid_pixels = ee.Image.constant(1).updateMask(mask).rename('valid_pixels')\n",
    "    \n",
    "    return image.updateMask(mask).addBands([total_pixels, valid_pixels])\n",
    "\n",
    "\n",
    "def add_indices(image):\n",
    "    \"\"\"Calculate NDVI, EVI, and Fractional Vegetation Cover\"\"\"\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    \n",
    "    nir = image.select('B8').divide(10000)\n",
    "    red = image.select('B4').divide(10000)\n",
    "    blue = image.select('B2').divide(10000)\n",
    "    \n",
    "    evi = nir.subtract(red).multiply(2.5).divide(\n",
    "        nir.add(red.multiply(6)).subtract(blue.multiply(7.5)).add(1)\n",
    "    ).rename('EVI')\n",
    "    \n",
    "    # Fractional Vegetation Cover\n",
    "    ndvi_soil = 0.2\n",
    "    ndvi_veg = 0.86\n",
    "    fvc = ndvi.subtract(ndvi_soil).divide(ndvi_veg - ndvi_soil).pow(2).clamp(0, 1).rename('FVC')\n",
    "    \n",
    "    return image.addBands([ndvi, evi, fvc])\n",
    "\n",
    "\n",
    "# Main processing\n",
    "start_date = '2025-04-01'\n",
    "end_date = '2025-10-31'\n",
    "\n",
    "s2 = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')\n",
    "      .filterDate(start_date, end_date)\n",
    "      .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "      .map(mask_s2_clouds)\n",
    "      .map(add_indices))\n",
    "\n",
    "# Create composite\n",
    "composite = s2.select(['NDVI', 'EVI', 'FVC']).median()\n",
    "\n",
    "# Pixel counts - sum across collection\n",
    "pixel_counts = s2.select(['total_pixels', 'valid_pixels']).sum()\n",
    "\n",
    "# Build combined reducer for all stats\n",
    "reducer = (\n",
    "    ee.Reducer.mean()\n",
    "    .combine(ee.Reducer.median(), sharedInputs=True)\n",
    "    .combine(ee.Reducer.stdDev(), sharedInputs=True)\n",
    "    .combine(ee.Reducer.max(), sharedInputs=True)\n",
    "    .combine(ee.Reducer.min(), sharedInputs=True)\n",
    "    .combine(ee.Reducer.count(), sharedInputs=True)\n",
    ")\n",
    "\n",
    "# Load the boundaries\n",
    "boundaries = ee.FeatureCollection('projects/ndvi-inspire/assets/boundaries_uk')\n",
    "\n",
    "print(f'Loaded {boundaries.size().getInfo()} features')\n",
    "\n",
    "# Extract statistics\n",
    "veg_stats = composite.reduceRegions(\n",
    "    collection=boundaries,\n",
    "    reducer=reducer,\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "pixel_stats = pixel_counts.reduceRegions(\n",
    "    collection=boundaries,\n",
    "    reducer=ee.Reducer.sum(),\n",
    "    scale=10\n",
    ")\n",
    "\n",
    "# Export to your GCS bucket\n",
    "task1 = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=veg_stats,\n",
    "    description='uk_veg_stats',\n",
    "    bucket='a-h-a-h',\n",
    "    fileNamePrefix='gee_exports/uk_veg_stats',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "\n",
    "task2 = ee.batch.Export.table.toCloudStorage(\n",
    "    collection=pixel_stats,\n",
    "    description='uk_pixel_counts',\n",
    "    bucket='a-h-a-h',\n",
    "    fileNamePrefix='gee_exports/uk_pixel_counts',\n",
    "    fileFormat='CSV'\n",
    ")\n",
    "\n",
    "task1.start()\n",
    "task2.start()\n",
    "\n",
    "print('Export tasks started')\n",
    "\n",
    "# Create the output directory for GEE exports if it doesn't exist\n",
    "output_dir = Path('data/raw_data/gee_exports')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the vegetation statistics CSV from Google Cloud Storage\n",
    "subprocess.run(['gsutil', 'cp', 'gs://a-h-a-h/gee_exports/uk_veg_stats.csv', str(output_dir)], check=True)\n",
    "# Download the pixel counts CSV from Google Cloud Storage\n",
    "subprocess.run(['gsutil', 'cp', 'gs://a-h-a-h/gee_exports/uk_pixel_counts.csv', str(output_dir)], check=True)\n",
    "\n",
    "# Read the downloaded CSV files into pandas DataFrames\n",
    "veg_stats = pd.read_csv(output_dir / 'uk_veg_stats.csv')\n",
    "pixel_counts = pd.read_csv(output_dir / 'uk_pixel_counts.csv')\n",
    "\n",
    "# Clean and merge\n",
    "veg_stats = veg_stats[['LSOA_DZ_SD', 'EVI_count', 'EVI_max', 'EVI_mean', 'EVI_median', 'EVI_min', 'EVI_stdDev', 'FVC_count', 'FVC_max', 'FVC_mean', 'FVC_median', 'FVC_min', 'FVC_stdDev', 'NDVI_count', 'NDVI_max', 'NDVI_mean', 'NDVI_median', 'NDVI_min', 'NDVI_stdDev']]\n",
    "pixel_counts = pixel_counts.drop(columns=['system:index', '.geo'])\n",
    "veg_stats = veg_stats.merge(pixel_counts, on='LSOA_DZ_SD', how='left')\n",
    "\n",
    "# Merge veg_stats into boundaries_all_UK by matching LSOA IDs\n",
    "boundaries_all_UK = boundaries_all_UK.merge(\n",
    "    veg_stats,\n",
    "    left_on=\"LSOA_DZ_SDZ_21_22\",\n",
    "    right_on=\"LSOA_DZ_SD\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"LSOA_DZ_SD\"], errors=\"ignore\")\n",
    "\n",
    "out_dir = Path(\"data\") / \"raw_data\" / \"gee_exports\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"Vegetation_Indicies_2025.parquet\"\n",
    "boundaries_all_UK.to_parquet(out_path, index=False)\n",
    "print(\"Saved geoparquet to\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a562ba6",
   "metadata": {},
   "source": [
    "### Monitoring the Task\n",
    "The following code can be run to monitor the status of the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dbd0b54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTAO676EZN5NHJ43QYTHUNKQ  Export.table  uk_pixel_counts                             SUCCEEDED  ---\n",
      "6CY63UC6N3BVDRI2MATOKVGM  Export.table  uk_veg_stats                                SUCCEEDED  ---\n",
      "2TKRAP2X4CRSS4ZXCZVAJJZT  Export.table  uk_pixel_counts                             SUCCEEDED  ---\n",
      "NP4ICG7KIZYPCOTNRT7SIKSZ  Export.table  uk_veg_stats                                SUCCEEDED  ---\n",
      "A3WF3I3BUPXU5YKLUU7JEK5A  Upload        Ingest table: \"projects/ndvi-inspire/ass..  SUCCEEDED  ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Monitor task progress\n",
    "import subprocess\n",
    "result = subprocess.run(['earthengine', 'task', 'list'], capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885a9c6",
   "metadata": {},
   "source": [
    "### Create AHAH Greenspace Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74046ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved greenspace geoparquet to data/green_blue/greenspace.parquet\n"
     ]
    }
   ],
   "source": [
    "ndvi_path = Path(\"data/raw_data/gee_exports/Vegetation_Indicies_2025.parquet\")\n",
    "df_ndvi_median = gpd.read_parquet(ndvi_path)\n",
    "df_ndvi_median = df_ndvi_median.rename(columns={\"LSOA_DZ_SD\": \"LSOA_DZ_SDZ_21_22\"})\n",
    "df_ndvi_median = df_ndvi_median[['LSOA_DZ_SDZ_21_22', 'geometry', 'NDVI_median']].copy()\n",
    "out_dir = Path(\"data\") / \"green_blue\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"greenspace.parquet\"\n",
    "df_ndvi_median.to_parquet(out_path, index=False)\n",
    "print(\"Saved greenspace geoparquet to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e269349e",
   "metadata": {},
   "source": [
    "## Active Greenspace\n",
    "The following code generates the active Greenspace indicator using Ordnace Survey Open Greenspace data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a310a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 54.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/github/access_to_cash_2025/.conda/lib/python3.12/site-packages/pyogrio/geopandas.py:382: UserWarning: More than one layer found in 'opgrsp_gb.gpkg': 'access_point' (default), 'greenspace_site'. Specify layer parameter to avoid this warning.\n",
      "  result = read_func(\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import tempfile\n",
    "import zipfile\n",
    "import os\n",
    "import geopandas as gpd\n",
    "\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "zip_path = os.path.join(tmp_dir, \"opgrsp_gpkg_gb.zip\")\n",
    "\n",
    "url = \"https://api.os.uk/downloads/v1/products/OpenGreenspace/downloads?area=GB&format=GeoPackage&redirect\"\n",
    "\n",
    "response = requests.get(url, allow_redirects=True, timeout=120)\n",
    "response.raise_for_status()\n",
    "\n",
    "print(f\"Downloaded {len(response.content) / 1024**2:.1f} MB\")\n",
    "\n",
    "with open(zip_path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(tmp_dir)\n",
    "\n",
    "gpkg_path = os.path.join(tmp_dir, \"Data\", \"opgrsp_gb.gpkg\")\n",
    "\n",
    "active_greenspace_polygons = gpd.read_file(gpkg_path, layer='greenspace_site')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "f1d2c940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined blue space points to data/green_blue/greenspace_active.parquet\n",
      "Total points: 2,582,449\n"
     ]
    }
   ],
   "source": [
    "active_greenspace_polygons = gpd.read_file(gpkg_path, layer='greenspace_site')\n",
    "\n",
    "# Convert LineStrings to points\n",
    "from shapely import get_coordinates\n",
    "\n",
    "coords = get_coordinates(active_greenspace_polygons.geometry)\n",
    "\n",
    "gb_green_points = gpd.GeoDataFrame(\n",
    "    geometry=gpd.points_from_xy(coords[:, 0], coords[:, 1]),\n",
    "    crs=active_greenspace_polygons.crs\n",
    ")\n",
    "\n",
    "gb_green_points = gb_green_points.to_crs('EPSG:4326')\n",
    "\n",
    "# Save to parquet\n",
    "green_space_dir = Path(\"data\") / \"green_blue\"\n",
    "green_space_dir.mkdir(parents=True, exist_ok=True)\n",
    "green_space_out_path = green_space_dir / \"greenspace_active.parquet\"\n",
    "gb_green_points.to_parquet(green_space_out_path, index=False)\n",
    "print(f\"Saved combined blue space points to {green_space_out_path}\")\n",
    "print(f\"Total points: {len(gb_green_points):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
