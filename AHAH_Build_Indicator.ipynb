{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1a28f4b",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afeb6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from scipy.stats import norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d859dc3",
   "metadata": {},
   "source": [
    "## Create the Accesibility Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fecf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_routing_results(\n",
    "    parquet_dir: str | Path,\n",
    "    group_col: str = 'lsoa21cd',\n",
    "    value_col: str = 'duration_minutes',\n",
    "    pattern: str = \"*.parquet\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate routing results from multiple parquet files into a single DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_dir : str or Path\n",
    "        Directory containing parquet files\n",
    "    group_col : str\n",
    "        Column to group by (default: 'lsoa21cd')\n",
    "    value_col : str\n",
    "        Column to compute mean of (default: 'duration_minutes')\n",
    "    pattern : str\n",
    "        Glob pattern for parquet files (default: '*.parquet')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with group_col as index and one column per input file\n",
    "    \"\"\"\n",
    "    parquet_dir = Path(parquet_dir)\n",
    "    parquet_files = sorted(parquet_dir.glob(pattern))\n",
    "    \n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found in {parquet_dir} matching '{pattern}'\")\n",
    "    \n",
    "    aggregated_dfs = []\n",
    "    \n",
    "    for path in parquet_files:\n",
    "        col_name = path.stem  # filename without extension\n",
    "        df = pd.read_parquet(path)\n",
    "        \n",
    "        agg_df = (\n",
    "            df.groupby(group_col, as_index=False)[value_col]\n",
    "            .mean()\n",
    "            .rename(columns={value_col: col_name})\n",
    "        )\n",
    "        aggregated_dfs.append(agg_df)\n",
    "    \n",
    "    # Merge all dataframes on group_col\n",
    "    result = aggregated_dfs[0]\n",
    "    for df in aggregated_dfs[1:]:\n",
    "        result = result.merge(df, on=group_col, how='outer')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ef6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all routing results\n",
    "Accessibility = aggregate_routing_results(\n",
    "    parquet_dir=\"data/routing_results/\",\n",
    "    group_col='lsoa21cd',\n",
    "    value_col='duration_minutes',\n",
    "    pattern=\"*.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52d8bd",
   "metadata": {},
   "source": [
    "# Air Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb253ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process NO2 data\n",
    "no2_path = Path(\"data/airquality/no2.parquet\")\n",
    "no2 = pd.read_parquet(no2_path)\n",
    "no2 = no2[['LSOA_DZ_SDZ_21_22', 'nox2024_weighted_mean']].rename(columns={'nox2024_weighted_mean': 'NO2'})\n",
    "\n",
    "# Load and process PM10 data\n",
    "pm10_path = Path(\"data/airquality/pm10.parquet\")\n",
    "pm10 = pd.read_parquet(pm10_path)\n",
    "pm10 = pm10[['LSOA_DZ_SDZ_21_22', 'pm102024g_weighted_mean']].rename(columns={'pm102024g_weighted_mean': 'PM10'})\n",
    "\n",
    "# Load and process SO2 data\n",
    "so2_path = Path(\"data/airquality/so2.parquet\")\n",
    "so2 = pd.read_parquet(so2_path)\n",
    "so2 = so2[['LSOA_DZ_SDZ_21_22', 'so22024_weighted_mean']].rename(columns={'so22024_weighted_mean': 'SO2'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d02405e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Air Quality data\n",
    "air_quality = no2.merge(pm10, on=\"LSOA_DZ_SDZ_21_22\", how=\"inner\").merge(so2, on=\"LSOA_DZ_SDZ_21_22\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccc470",
   "metadata": {},
   "source": [
    "# Green Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8f916cb",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Passive Greenaspace data\n",
    "greenspace_path = Path(\"data/green_blue/greenspace.parquet\")\n",
    "greenspace = gpd.read_parquet(greenspace_path)\n",
    "greenspace = greenspace[['LSOA_DZ_SDZ_21_22', 'NDVI_median']].rename(columns={'NDVI_median': 'greenspace'})\n",
    "greenspace = greenspace[greenspace['LSOA_DZ_SDZ_21_22'].isin(air_quality['LSOA_DZ_SDZ_21_22'])] # Removes the NI data\n",
    "greenspace = greenspace.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472eb01",
   "metadata": {},
   "source": [
    "# Gather All the Input Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bbee63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lsoa21cd', 'GP', 'bluespace', 'dentist', 'fast_food', 'gambling',\n",
       "       'greenspace_active', 'hospital', 'leisure', 'pharmacy', 'pub_bar',\n",
       "       'tobacco', 'NO2', 'PM10', 'SO2', 'greenspace'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge Accessibility with air quality and greenspace using the LSOA code\n",
    "aq = air_quality.rename(columns={'LSOA_DZ_SDZ_21_22': 'lsoa21cd'})\n",
    "gs = greenspace.rename(columns={'LSOA_DZ_SDZ_21_22': 'lsoa21cd'})\n",
    "\n",
    "AHAH_input = Accessibility.merge(aq, on='lsoa21cd', how='left').merge(gs, on='lsoa21cd', how='left')\n",
    "AHAH_input.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd45725",
   "metadata": {},
   "source": [
    "# Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7cd35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def exp_trans(x, n):\n",
    "    \"\"\"Exponential transformation for domain scores.\"\"\"\n",
    "    return -23 * np.log(1 - (x / n) * (1 - np.exp(-100 / 23)))\n",
    "\n",
    "\n",
    "def exp_default(x, n):\n",
    "    \"\"\"Probit transformation for individual indicators.\"\"\"\n",
    "    return norm.ppf((x - 0.5) / n)\n",
    "\n",
    "\n",
    "def calculate_ahah(df):\n",
    "    \"\"\"\n",
    "    Calculate AHAH index from raw distance/accessibility measures.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Must contain columns: lsoa21cd, GP, dentist, pharmacy, hospital, leisure,\n",
    "        greenspace, bluespace, NO2, PM10, SO2, fast_food, gambling, pub_bar, tobacco\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Original data with added rank, percentile, domain, and AHAH scores\n",
    "    \"\"\"\n",
    "    idx = df.copy()\n",
    "    n = len(idx)\n",
    "    \n",
    "    # Define indicator groups by domain\n",
    "    health = [\"GP\", \"dentist\", \"pharmacy\", \"hospital\", \"leisure\"]\n",
    "    green_blue = [\"greenspace\",\"greenspace_active\", \"bluespace\"]\n",
    "    air_quality = [\"NO2\", \"PM10\", \"SO2\"]\n",
    "    retail = [\"fast_food\", \"gambling\", \"pub_bar\", \"tobacco\"]\n",
    "    \n",
    "    # --- RANKING ---\n",
    "    # Health services: lower distance = better access (rank ascending)\n",
    "    for col in health:\n",
    "        idx[f\"{col}_rnk\"] = idx[col].rank(method=\"min\").astype(int)\n",
    "    \n",
    "    # Active greenspace: lower distance = better (rank ascending)\n",
    "    idx[\"greenspace_active_rnk\"] = idx[\"greenspace_active\"].rank(method=\"min\").astype(int)\n",
    "\n",
    "    # Greenspace: higher NDVI = better (rank descending)\n",
    "    idx[\"greenspace_rnk\"] = idx[\"greenspace\"].rank(method=\"min\", ascending=False).astype(int)\n",
    "    \n",
    "    # Bluespace: lower distance = better (rank ascending)\n",
    "    idx[\"bluespace_rnk\"] = idx[\"bluespace\"].rank(method=\"min\").astype(int)\n",
    "    \n",
    "    # Air quality: lower pollution = better (rank ascending)\n",
    "    for col in air_quality:\n",
    "        idx[f\"{col}_rnk\"] = idx[col].rank(method=\"min\").astype(int)\n",
    "    \n",
    "    # Retail hazards: higher distance from hazards = better (rank descending)\n",
    "    for col in retail:\n",
    "        idx[f\"{col}_rnk\"] = idx[col].rank(method=\"min\", ascending=False).astype(int)\n",
    "    \n",
    "    # --- PROBIT TRANSFORMATION (normalizes ranked indicators) ---\n",
    "    all_indicators = health + green_blue + air_quality + retail\n",
    "    for col in all_indicators:\n",
    "        idx[f\"{col}_exp\"] = exp_default(idx[f\"{col}_rnk\"], n)\n",
    "    \n",
    "    # --- PERCENTILES (for each indicator) ---\n",
    "    for col in all_indicators:\n",
    "        idx[f\"{col}_pct\"] = (idx[f\"{col}_rnk\"] / idx[f\"{col}_rnk\"].max() * 100).astype(int)\n",
    "    \n",
    "    # --- DOMAIN SCORES (mean of transformed indicators per domain) ---\n",
    "    idx[\"domain_h\"] = idx[[f\"{col}_exp\" for col in health]].mean(axis=1)         # Health\n",
    "    idx[\"domain_g\"] = idx[[f\"{col}_exp\" for col in green_blue]].mean(axis=1)     # Green/Blue\n",
    "    idx[\"domain_e\"] = idx[[f\"{col}_exp\" for col in air_quality]].mean(axis=1)    # Environment\n",
    "    idx[\"domain_r\"] = idx[[f\"{col}_exp\" for col in retail]].mean(axis=1)         # Retail hazards\n",
    "    \n",
    "    # --- DOMAIN RANKS AND PERCENTILES ---\n",
    "    for domain in [\"h\", \"g\", \"e\", \"r\"]:\n",
    "        idx[f\"domain_{domain}_rnk\"] = idx[f\"domain_{domain}\"].rank(method=\"min\").astype(int)\n",
    "        idx[f\"domain_{domain}_pct\"] = pd.qcut(idx[f\"domain_{domain}_rnk\"], 100, labels=False) + 1\n",
    "    \n",
    "    # --- FINAL AHAH SCORE (exponential transform of domain ranks, then average) ---\n",
    "    idx[\"h_exp\"] = exp_trans(idx[\"domain_h_rnk\"], n)\n",
    "    idx[\"g_exp\"] = exp_trans(idx[\"domain_g_rnk\"], n)\n",
    "    idx[\"e_exp\"] = exp_trans(idx[\"domain_e_rnk\"], n)\n",
    "    idx[\"r_exp\"] = exp_trans(idx[\"domain_r_rnk\"], n)\n",
    "    \n",
    "    # Composite AHAH score (equal domain weights)\n",
    "    idx[\"ahah\"] = idx[[\"h_exp\", \"g_exp\", \"e_exp\", \"r_exp\"]].mean(axis=1)\n",
    "    idx[\"ahah_rnk\"] = idx[\"ahah\"].rank(method=\"min\").astype(int)\n",
    "    idx[\"ahah_pct\"] = pd.qcut(idx[\"ahah_rnk\"], 100, labels=False) + 1\n",
    "    \n",
    "    # Remove intermediate exponential columns\n",
    "    exp_cols = [c for c in idx.columns if c.endswith(\"_exp\")]\n",
    "    idx = idx.drop(columns=exp_cols)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Calculate AHAH index using the prepared input data\n",
    "AHAH_V5 = calculate_ahah(AHAH_input)\n",
    "AHAH_V5\n",
    "\n",
    "AHAH_V5.to_csv('./AHAH_V5.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1cf5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Load boundary data and merge with AHAH_V5\n",
    "boundary_path = Path(\"data/boundary/LSOA_DZ_SDZ_21_22.parquet\")\n",
    "boundary = pd.read_parquet(boundary_path)\n",
    "\n",
    "# Merge AHAH_V5 with boundary data\n",
    "AHAH_V5_geo = boundary.merge(AHAH_V5, left_on='LSOA_DZ_SDZ_21_22', right_on='lsoa21cd', how='inner')\n",
    "AHAH_V5_geo\n",
    "\n",
    "# Convert to GeoDataFrame and export as GeoParquet\n",
    "gdf = gpd.GeoDataFrame(AHAH_V5_geo, geometry=gpd.GeoSeries.from_wkb(AHAH_V5_geo['geometry']))\n",
    "gdf.to_parquet(\"AHAH_V5_geo.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
